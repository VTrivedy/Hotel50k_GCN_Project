{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precompute Validation Subgraphs\n",
    "This is a simple script go from our validation dictionary (containing nodes, neighbors, features, and labels) to a list of subgraphs on which we can evaluate.  To do this we will do the following (hopefully in parallel):\n",
    "- Insert each node into the unsupervised graph one node at a time\n",
    "- After inserting a node, extract its 2-hop subgraph\n",
    "- Save each 2-hop subgraph in a list --> we will need it for evaluation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import dgl\n",
    "import scipy\n",
    "import networkx as nx\n",
    "from progressbar import progressbar\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from dgl.data.utils import save_graphs, load_graphs, split_dataset\n",
    "\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.utils import io\n",
    "\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Store Subgraph Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this needs to operate on a single validation node and its data\n",
    "def compute_subgraph(dict_entry):\n",
    "    temp_graphs, temp_labels = load_graphs(\"new_train_graphs.bin\")\n",
    "    temp_unsup_graph = temp_graphs[0]\n",
    "    \n",
    "    #extract our graph\n",
    "    #temp_unsup_graph = entry[0]\n",
    "    #temp_unsup_graph = temp_unsup_graph.add_self_loop()\n",
    "    \n",
    "    #extract our validation node data\n",
    "    #dict_entry = entry[1]\n",
    "    \n",
    "    #extract node features, label, neighbors from validation entry\n",
    "    temp_feats = th.tensor(dict_entry['features'])\n",
    "    temp_label = th.tensor(dict_entry['label'])\n",
    "    neighb_list = dict_entry['neighbors']\n",
    "    \n",
    "    #add node to graph\n",
    "    temp_unsup_graph.add_nodes(1) # this is saying add 1 node --> it will always be node 399722\n",
    "    temp_unsup_graph.ndata['labels'][-1] = temp_label\n",
    "    temp_unsup_graph.ndata['features'][-1] = temp_feats\n",
    "    \n",
    "    assert temp_unsup_graph.number_of_nodes() == 399723\n",
    "    #store edges and their reverse\n",
    "    val_neighbors = [[399722, x] for x in neighb_list]\n",
    "    val_neighbors_reversed = [[x, 399722] for x in neighb_list]\n",
    "    final_val_neighbors = [*val_neighbors, *val_neighbors_reversed]\n",
    "    source_nodes = [x[0] for x in final_val_neighbors]\n",
    "    dest_nodes = [x[1] for x in final_val_neighbors]\n",
    "    \n",
    "    #add edges\n",
    "    temp_unsup_graph.add_edges(source_nodes, dest_nodes)\n",
    "\n",
    "    #define our sampler and dataloader --> we will use this to extract the subgraph around our validation node\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([15,10])\n",
    "    temp_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    temp_unsup_graph, [399722], sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0)\n",
    "\n",
    "    #get our sampled subgraph\n",
    "    temp_input_nodes, temp_output_nodes, temp_blocks = next(iter(temp_dataloader))\n",
    "\n",
    "    #check to ensure that our output node is what we want it to be\n",
    "#     assert th.equal(temp_blocks[-1].dstdata['labels'][-1], th.tensor(validation_dict[nid]['label'])) #check that the labels match\n",
    "#     assert th.equal(temp_blocks[-1].dstdata['features'].flatten(), th.tensor(validation_dict[nid]['features'])) #check that the features match\n",
    "    \n",
    "    return (temp_input_nodes, temp_output_nodes, temp_blocks) #return tuple all the necessary info for our validation subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                     #                          | 1999 Elapsed Time: 0:04:26\n",
      "| |            #                                   | 1999 Elapsed Time: 0:04:27\n",
      "| |               #                                | 1999 Elapsed Time: 0:04:27\n",
      "| |                                      #         | 1999 Elapsed Time: 0:04:24\n",
      "| |                                         #      | 1999 Elapsed Time: 0:04:23\n",
      "| |                               #                | 1999 Elapsed Time: 0:04:22\n",
      "| |                                         #      | 1999 Elapsed Time: 0:04:24\n",
      "| |                   #                            | 1999 Elapsed Time: 0:04:21\n",
      "| |            #                                   | 1999 Elapsed Time: 0:04:20\n",
      "| |           #                                    | 1999 Elapsed Time: 0:04:20\n",
      "| |                         #                      | 1999 Elapsed Time: 0:04:21\n",
      "| |            #                                   | 1999 Elapsed Time: 0:04:20\n",
      "| |                                       #        | 1999 Elapsed Time: 0:04:23\n",
      "| |                       #                        | 1999 Elapsed Time: 0:04:21\n",
      "| |                                       #        | 1999 Elapsed Time: 0:04:24\n",
      "| |       #                                        | 1999 Elapsed Time: 0:04:19\n",
      "| |                                   #            | 1999 Elapsed Time: 0:04:22\n",
      "| |                              #                 | 1999 Elapsed Time: 0:04:25\n",
      "| |                              #                 | 1999 Elapsed Time: 0:04:25\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #load in validation dictionary\n",
    "    with open('final_validation_data.pickle', 'rb') as f:\n",
    "        validation_dict = pickle.load(f)\n",
    "    \n",
    "    validation_values = list(validation_dict.values())\n",
    "    \n",
    "    #split our subgraphs into batches --> if not the parallel process mems out after a while\n",
    "    BATCH_SIZE = 2000\n",
    "    node_batches = [validation_values[i:i + BATCH_SIZE] for i in range(0, len(validation_values), BATCH_SIZE)]\n",
    "    #val_data_graph = [(temp_unsup_graph.clone(), x) for x in validation_values]\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=50) as executor:\n",
    "        for i, batch in enumerate(node_batches[1:]):\n",
    "            result = []\n",
    "            for r in progressbar(executor.map(compute_subgraph, batch)):\n",
    "                result.append(r)\n",
    "            th.save(result, f'val_subgraphs/subgraph{i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Mini Subgraphs into Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all of our subgraph calculations into one file\n",
    "full_subgraph_list = []\n",
    "for i in range(20):\n",
    "    x = th.load(f'val_subgraphs/subgraph{i}')\n",
    "    full_subgraph_list.extend(x)\n",
    "\n",
    "#save subgraphs\n",
    "th.save(full_subgraph_list, f'val_subgraphs/full_subgraph_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in subgraphs to check that saving and loading is ok\n",
    "all_subgraphs = th.load('val_subgraphs/full_subgraph_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure the labels match up\n",
    "subgraph_labels = []\n",
    "for sg in all_subgraphs:\n",
    "    subgraph_labels.append(sg[2][-1].dstdata['labels'].item())\n",
    "    \n",
    "val_dict_labels = []\n",
    "for val in list(validation_dict.values()):\n",
    "    val_dict_labels.append(val['label'])\n",
    "    \n",
    "assert subgraph_labels == val_dict_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code - Unparallelized\n",
    "Keeping in case need for reference later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precompute_validation_subgraphs(validation_dict):\n",
    "#     subgraph_holder = [] #we will store all of our subgraphs here\n",
    "    \n",
    "#     keys = list(validation_dict.keys())[0:100] #keys to our validation nodes\n",
    "#     for nid in progressbar(keys):\n",
    "#         #temporary copy of graph that we will pass in our new nodes to\n",
    "#         temp_graphs, temp_labels = load_graphs(\"new_train_graphs.bin\")\n",
    "#         temp_unsup_graph = temp_graphs[0]\n",
    "#         temp_unsup_graph = temp_unsup_graph.add_self_loop()\n",
    "\n",
    "#         #add node from validation set to our graph and extract its features, label, and neighbors\n",
    "#         temp_feats = th.tensor(validation_dict[nid]['features'])\n",
    "#         temp_label = th.tensor(validation_dict[nid]['label'])\n",
    "\n",
    "#         #add node to graph\n",
    "#         temp_unsup_graph.add_nodes(1) # this is saying add 1 node --> it will always be node 399722\n",
    "#         temp_unsup_graph.ndata['labels'][-1] = temp_label\n",
    "#         temp_unsup_graph.ndata['features'][-1] = temp_feats\n",
    "\n",
    "#         #store edges and their reverse\n",
    "#         neighb_list = validation_dict[nid]['neighbors']\n",
    "#         val_neighbors = [[399722, x] for x in neighb_list]\n",
    "#         val_neighbors_reversed = [[x, 399722] for x in neighb_list]\n",
    "#         final_val_neighbors = [*val_neighbors, *val_neighbors_reversed]\n",
    "#         source_nodes = [x[0] for x in final_val_neighbors]\n",
    "#         dest_nodes = [x[1] for x in final_val_neighbors]\n",
    "\n",
    "#         #add edges\n",
    "#         temp_unsup_graph.add_edges(source_nodes, dest_nodes)\n",
    "        \n",
    "#         #define our sampler and dataloader --> we will use this to extract the subgraph around our validation node\n",
    "#         sampler = dgl.dataloading.MultiLayerNeighborSampler([15,10])\n",
    "#         temp_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "#         temp_unsup_graph, [399722], sampler,\n",
    "#         batch_size=1024,\n",
    "#         shuffle=True,\n",
    "#         drop_last=False,\n",
    "#         num_workers=0)\n",
    "\n",
    "#         #get our sampled subgraph\n",
    "#         temp_input_nodes, temp_output_nodes, temp_blocks = next(iter(temp_dataloader))\n",
    "        \n",
    "#         #check to ensure that our output node is what we want it to be\n",
    "#         assert th.equal(temp_blocks[-1].dstdata['labels'][-1], th.tensor(validation_dict[nid]['label'])) #check that the labels match\n",
    "#         assert th.equal(temp_blocks[-1].dstdata['features'].flatten(), th.tensor(validation_dict[nid]['features'])) #check that the features match\n",
    "        \n",
    "#         #append subgraph info for each validation node as a tuple to a running list\n",
    "#         subgraph_holder.append((temp_input_nodes, temp_output_nodes, temp_blocks))\n",
    "        \n",
    "#     return subgraph_holder\n",
    "# subgraph_holder = precompute_validation_subgraphs(validation_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
