{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch GNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import dgl\n",
    "import scipy\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dgl.data.utils import save_graphs, load_graphs, split_dataset\n",
    "\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our graphs from before\n",
    "glist, label_dict = load_graphs(\"./data_final.bin\")\n",
    "unsup_graph, sup_graph = glist[0], glist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix problem where we have a label of -1 to represent the missing class\n",
    "tempLabels = sup_graph.ndata['label']\n",
    "\n",
    "#something weird - no labels from original data had class 8??? --> map our -1 class to 8 so it works with cross-entropy loss\n",
    "tempLabels[tempLabels==-1] = 8\n",
    "\n",
    "sup_graph.ndata['label'] = tempLabels\n",
    "unsup_graph.ndata['label'] = tempLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add self loops to both graphs\n",
    "sup_graph = sup_graph.add_self_loop()\n",
    "unsup_graph = unsup_graph.add_self_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tug57226/.local/lib/python3.6/site-packages/dgl/base.py:45: DGLWarning: DGLGraph.__len__ is deprecated.Please directly call DGLGraph.number_of_nodes.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#split data into train,test,val\n",
    "sup_split = split_dataset(sup_graph, shuffle=True, random_state=10)\n",
    "unsup_split = split_dataset(sup_graph, shuffle=True, random_state=10)\n",
    "\n",
    "#extract the splits\n",
    "sup_train, sup_val, sup_test = sup_split[0], sup_split[1], sup_split[2]\n",
    "unsup_train, unsup_val, unsup_test = unsup_split[0], unsup_split[1], unsup_split[2]\n",
    "\n",
    "#convert the index based representation into boolean masks for the graphs\n",
    "n = len(sup_graph) #total num nodes in each graph\n",
    "train_mask, val_mask, test_mask = np.zeros(n, dtype=bool), np.zeros(n, dtype=bool), np.zeros(n, dtype=bool) #create empty arrays for train/val/test\n",
    "\n",
    "#populate our boolean masks\n",
    "train_mask[sup_train.indices] = True \n",
    "val_mask[sup_val.indices] = True\n",
    "test_mask[sup_test.indices] = True\n",
    "\n",
    "#embed these masks into our graph\n",
    "sup_graph.ndata['train_mask'], sup_graph.ndata['val_mask'], sup_graph.ndata['test_mask'] = th.tensor(train_mask, dtype=bool), th.tensor(val_mask, dtype=bool), th.tensor(test_mask, dtype=bool)\n",
    "unsup_graph.ndata['train_mask'], unsup_graph.ndata['val_mask'], unsup_graph.ndata['test_mask'] = th.tensor(train_mask), th.tensor(val_mask), th.tensor(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and store information from each graph\n",
    "\n",
    "#sup\n",
    "sup_node_features = sup_graph.ndata['features']\n",
    "sup_node_labels = sup_graph.ndata['label']\n",
    "\n",
    "#unsup\n",
    "unsup_node_features = unsup_graph.ndata['features']\n",
    "unsup_node_labels = unsup_graph.ndata['label']\n",
    "\n",
    "#general graph characteristics - doesn't matter which graph\n",
    "train_mask = sup_graph.ndata['train_mask']\n",
    "valid_mask = sup_graph.ndata['val_mask']\n",
    "test_mask = sup_graph.ndata['test_mask']\n",
    "n_features = sup_node_features.shape[1]\n",
    "n_labels = int(sup_node_labels.max().item() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the graph into training graph, validation graph, and test graph by training\n",
    "#and validation masks.  Suitable for inductive models.\n",
    "\n",
    "#sup\n",
    "sup_train_graph = sup_graph.subgraph(sup_graph.ndata['train_mask'])\n",
    "sup_val_graph = sup_graph.subgraph(sup_graph.ndata['train_mask'] | sup_graph.ndata['val_mask'])\n",
    "sup_test_graph = sup_graph\n",
    "\n",
    "#unsup\n",
    "unsup_train_graph = unsup_graph.subgraph(unsup_graph.ndata['train_mask'])\n",
    "unsup_val_graph = unsup_graph.subgraph(unsup_graph.ndata['train_mask'] | unsup_graph.ndata['val_mask'])\n",
    "unsup_test_graph = unsup_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import time\n",
    "import argparse\n",
    "from _thread import start_new_thread\n",
    "from functools import wraps\n",
    "from dgl.data import RedditDataset\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h = layer(block, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        h = F.softmax(h,dim=-1)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, x, batch_size):\n",
    "        \"\"\"\n",
    "        Inference with the GraphSAGE model on full neighbors (i.e. without neighbor sampling).\n",
    "        g : the entire graph.\n",
    "        x : the input of entire node set.\n",
    "        The inference code is written in a fashion that it could handle any number of nodes and\n",
    "        layers.\n",
    "        \"\"\"\n",
    "        # During inference with sampling, multi-layer blocks are very inefficient because\n",
    "        # lots of computations in the first few layers are repeated.\n",
    "        # Therefore, we compute the representation of all nodes layer by layer.  The nodes\n",
    "        # on each layer are of course splitted in batches.\n",
    "        # TODO: can we standardize this?\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = th.zeros(g.number_of_nodes(), self.n_hidden if l != len(self.layers) - 1 else self.n_classes)\n",
    "\n",
    "            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "            dataloader = dgl.dataloading.NodeDataLoader(\n",
    "                g,\n",
    "                th.arange(g.number_of_nodes()),\n",
    "                sampler,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=False)\n",
    "\n",
    "            for input_nodes, output_nodes, blocks in tqdm(dataloader):\n",
    "                block = blocks[0]\n",
    "\n",
    "                block = block.int()\n",
    "                h = x[input_nodes]\n",
    "                h = layer(block, h)\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = self.activation(h)\n",
    "                    h = self.dropout(h)\n",
    "                h = F.softmax(h,dim=-1)\n",
    "\n",
    "                y[output_nodes] = h\n",
    "\n",
    "            x = y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def compute_acc(pred, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of prediction given the labels.\n",
    "    \"\"\"\n",
    "    labels = labels.long()\n",
    "    return (th.argmax(pred, dim=1) == labels).float().sum() / len(pred)\n",
    "\n",
    "def evaluate(model, g, inputs, labels, val_nid, batch_size):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set specified by ``val_nid``.\n",
    "    g : The entire graph.\n",
    "    inputs : The features of all the nodes.\n",
    "    labels : The labels of all the nodes.\n",
    "    val_nid : the node Ids for validation.\n",
    "    batch_size : Number of nodes to compute at the same time.\n",
    "    device : The GPU device to evaluate on.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        pred = model.inference(g, inputs, batch_size)\n",
    "    model.train()\n",
    "    return compute_acc(pred[val_nid], labels[val_nid])\n",
    "\n",
    "# def load_subtensor(g, seeds, input_nodes, device):\n",
    "#     \"\"\"\n",
    "#     Copys features and labels of a set of nodes onto GPU.\n",
    "#     \"\"\"\n",
    "#     batch_inputs = g.ndata['features'][input_nodes].to(device)\n",
    "#     batch_labels = g.ndata['labels'][seeds].to(device)\n",
    "#     return batch_inputs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run code\n",
    "#### Entry point\n",
    "def run(args, sup_data, unsup_data):\n",
    "    # Unpack data - sup and unsup\n",
    "    sup_in_feats, sup_n_classes, sup_train_g, sup_val_g, sup_test_g = sup_data\n",
    "    unsup_in_feats, unsup_n_classes, unsup_train_g, unsup_val_g, unsup_test_g = unsup_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    #get node IDs for train/text/val sets\n",
    "    sup_train_nid = th.nonzero(sup_train_g.ndata['train_mask'], as_tuple=True)[0]\n",
    "    sup_val_nid = th.nonzero(sup_val_g.ndata['val_mask'], as_tuple=True)[0]\n",
    "    sup_test_nid = th.nonzero(~(sup_test_g.ndata['train_mask'] | sup_test_g.ndata['val_mask']), as_tuple=True)[0]\n",
    "    \n",
    "    unsup_train_nid = th.nonzero(unsup_train_g.ndata['train_mask'], as_tuple=True)[0]\n",
    "    unsup_val_nid = th.nonzero(unsup_val_g.ndata['val_mask'], as_tuple=True)[0]\n",
    "    unsup_test_nid = th.nonzero(~(unsup_test_g.ndata['train_mask'] | unsup_test_g.ndata['val_mask']), as_tuple=True)[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create PyTorch DataLoaders for constructing blocks\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([15,10])\n",
    "    \n",
    "    sup_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        sup_train_g,\n",
    "        sup_train_nid,\n",
    "        sampler,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False)\n",
    "    \n",
    "    unsup_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        unsup_train_g,\n",
    "        unsup_train_nid,\n",
    "        sampler,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False)\n",
    "    \n",
    "    \n",
    "    # Define model and optimizer\n",
    "    model = SAGE(in_feats, args.num_hidden, n_classes, args.num_layers, F.relu, args.dropout)\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Training loop\n",
    "    avg = 0\n",
    "    iter_tput = []\n",
    "    for epoch in range(args.num_epochs):\n",
    "        tic = time.time()\n",
    "        \n",
    "#         if epoch == 0: # debugging\n",
    "#             continue\n",
    "\n",
    "        # Loop over the dataloader to sample the computation dependency graph as a list of\n",
    "        # blocks.\n",
    "        tic_step = time.time()\n",
    "        for step, dl in enumerate(zip(sup_dataloader,unsup_dataloader)):\n",
    "            #extract inputs from both data loaders\n",
    "            (sup_input_nodes, sup_seeds, sup_blocks) = dl[0]\n",
    "            (unsup_input_nodes, unsup_seeds, unsup_blocks) = dl[0]\n",
    "            \n",
    "            #test that each batch of nodes is the same\n",
    "            assert th.equal(sup_input_nodes,unsup_input_nodes)\n",
    "            assert th.equal(sup_seeds, unsup_seeds)\n",
    "            \n",
    "            \n",
    "            # Load the input features as well as output labels\n",
    "            #batch_inputs, batch_labels = load_subtensor(train_g, seeds, input_nodes, device)\n",
    "            sup_blocks = [block.int() for block in sup_blocks]\n",
    "            sup_batch_inputs = sup_blocks[0].srcdata['features']\n",
    "            sup_batch_labels = sup_blocks[-1].dstdata['label']\n",
    "            \n",
    "            unsup_blocks = [block.int() for block in unsup_blocks]\n",
    "            unsup_batch_inputs = unsup_blocks[0].srcdata['features']\n",
    "            unsup_batch_labels = unsup_blocks[-1].dstdata['label']\n",
    "\n",
    "            \n",
    "            \n",
    "            # Compute loss for each graph and add in additional term\n",
    "            sup_batch_pred = model(sup_blocks, sup_batch_inputs)\n",
    "            sup_loss = loss_fcn(sup_batch_pred, sup_batch_labels)\n",
    "            \n",
    "            unsup_batch_pred = model(unsup_blocks, unsup_batch_inputs)\n",
    "            unsup_loss = loss_fcn(unsup_batch_pred, unsup_batch_labels)\n",
    "            \n",
    "            additional_term = th.dist(sup_batch_pred, unsup_batch_pred,2)/args.batch_size\n",
    "            \n",
    "            \n",
    "            \n",
    "            #final loss and backprop\n",
    "            loss = sup_loss + unsup_loss + additional_term\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iter_tput.append(len(sup_seeds) / (time.time() - tic_step))\n",
    "            if step % args.log_every == 0:\n",
    "                acc = compute_acc(sup_batch_pred, sup_batch_labels)\n",
    "                print('Epoch {:05d} | Step {:09d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f}'.format(\n",
    "                    epoch, step, loss.item(), acc.item(), np.mean(iter_tput[3:])))\n",
    "                with open('./logs/log_file.txt', 'a') as f:\n",
    "                    f.write('Epoch {:05d} | Step {:09d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f}\\n'.format(\n",
    "                    epoch, step, loss.item(), acc.item(), np.mean(iter_tput[3:])))\n",
    "            tic_step = time.time()\n",
    "\n",
    "        toc = time.time()\n",
    "        print('Epoch Time(s): {:.4f}'.format(toc - tic))\n",
    "        \n",
    "        with open('./logs/log_file.txt', 'a') as f:\n",
    "            f.write('Epoch Time(s): {:.4f}\\n\\n'.format(toc - tic))\n",
    "            f.write('________________________________________________\\n\\n\\n')\n",
    "        \n",
    "        if epoch >= 5:\n",
    "            avg += toc - tic\n",
    "        if epoch % args.eval_every == 0 and epoch != 0:\n",
    "            eval_acc = evaluate(model, unsup_val_g, unsup_val_g.ndata['features'], unsup_val_g.ndata['label'], unsup_val_nid, args.batch_size)\n",
    "            print('Eval Acc {:.4f}'.format(eval_acc))\n",
    "            test_acc = evaluate(model, unsup_test_g, unsup_test_g.ndata['features'], unsup_test_g.ndata['label'], unsup_test_nid, args.batch_size)\n",
    "            print('Test Acc: {:.4f}'.format(test_acc))\n",
    "            \n",
    "            with open('./logs/log_file.txt', 'a') as f:\n",
    "                f.write('________________________________________________\\n\\n\\n')\n",
    "                f.write('Eval Acc {:.4f}\\n'.format(eval_acc))\n",
    "                f.write('Test Acc: {:.4f}\\n'.format(test_acc))\n",
    "                f.write('________________________________________________\\n\\n\\n')\n",
    "                \n",
    "                \n",
    "            \n",
    "        th.save(model.state_dict(), f'./mini_batch_models/model{epoch}')\n",
    "\n",
    "    #print('Avg epoch time: {}'.format(avg / (epoch - 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Step 000000000 | Loss 9.1108 | Train Acc 0.0088 | Speed (samples/sec) nan\n",
      "Epoch 00000 | Step 000000100 | Loss 7.9164 | Train Acc 0.5928 | Speed (samples/sec) 1695.4224\n",
      "Epoch 00000 | Step 000000200 | Loss 7.8910 | Train Acc 0.6055 | Speed (samples/sec) 1723.2751\n",
      "Epoch 00000 | Step 000000300 | Loss 7.9281 | Train Acc 0.5869 | Speed (samples/sec) 1734.3549\n"
     ]
    }
   ],
   "source": [
    "class arg_holder():\n",
    "    def __init__(self, num_epochs=20, num_hidden=256, num_layers=2, batch_size=1024, log_every=100, eval_every=1, lr=0.003, dropout=0.5):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers= num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.log_every = log_every\n",
    "        self.eval_every = eval_every\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "# argparser = argparse.ArgumentParser(\"mini-batch training\")\n",
    "# argparser.add_argument('--num-epochs', type=int, default=20)\n",
    "# argparser.add_argument('--num-hidden', type=int, default=256)\n",
    "# argparser.add_argument('--num-layers', type=int, default=2)\n",
    "# argparser.add_argument('--batch-size', type=int, default=1000)\n",
    "# argparser.add_argument('--log-every', type=int, default=4)\n",
    "# argparser.add_argument('--eval-every', type=int, default=4)\n",
    "# argparser.add_argument('--lr', type=float, default=0.003)\n",
    "# argparser.add_argument('--dropout', type=float, default=0.5)\n",
    "# args = argparser.parse_args()\n",
    "\n",
    "args = arg_holder()\n",
    "\n",
    "#get dimension of embedded node features and number of classes\n",
    "in_feats = sup_graph.ndata['features'].shape[1]\n",
    "n_classes = len(set(unsup_graph.ndata['label'].tolist()))\n",
    "\n",
    "sup_data = in_feats, n_classes, sup_train_graph, sup_val_graph, sup_test_graph\n",
    "unsup_data = in_feats, n_classes, unsup_train_graph, unsup_val_graph, unsup_test_graph\n",
    "\n",
    "run(args, sup_data, unsup_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data - sup and unsup\n",
    "sup_in_feats, sup_n_classes, sup_train_g, sup_val_g, sup_test_g = sup_data\n",
    "unsup_in_feats, unsup_n_classes, unsup_train_g, unsup_val_g, unsup_test_g = unsup_data\n",
    "\n",
    "\n",
    "\n",
    "#get node IDs for train/val/test sets\n",
    "sup_train_nid = th.nonzero(sup_train_g.ndata['train_mask'], as_tuple=True)[0]\n",
    "sup_val_nid = th.nonzero(sup_val_g.ndata['val_mask'], as_tuple=True)[0]\n",
    "sup_test_nid = th.nonzero(~(sup_test_g.ndata['train_mask'] | sup_test_g.ndata['val_mask']), as_tuple=True)[0]\n",
    "\n",
    "unsup_train_nid = th.nonzero(unsup_train_g.ndata['train_mask'], as_tuple=True)[0]\n",
    "unsup_val_nid = th.nonzero(unsup_val_g.ndata['val_mask'], as_tuple=True)[0]\n",
    "unsup_test_nid = th.nonzero(~(unsup_test_g.ndata['train_mask'] | unsup_test_g.ndata['val_mask']), as_tuple=True)[0]\n",
    "\n",
    "\n",
    "\n",
    "# Create PyTorch DataLoaders for constructing blocks\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([15,10])\n",
    "\n",
    "sup_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    sup_train_g,\n",
    "    sup_train_nid,\n",
    "    sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False)\n",
    "\n",
    "unsup_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    unsup_train_g,\n",
    "    unsup_train_nid,\n",
    "    sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and optimizer\n",
    "model = SAGE(in_feats, args.num_hidden, n_classes, args.num_layers, F.relu, args.dropout)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "for step, dl in enumerate(zip(sup_dataloader,unsup_dataloader)):\n",
    "            #extract inputs from both data loaders\n",
    "            (sup_input_nodes, sup_seeds, sup_blocks) = dl[0]\n",
    "            (unsup_input_nodes, unsup_seeds, unsup_blocks) = dl[0]\n",
    "            \n",
    "            assert th.equal(sup_input_nodes,unsup_input_nodes)\n",
    "            assert th.equal(sup_seeds, unsup_seeds)\n",
    "            print(sup_input_nodes, '\\n', sup_seeds, '\\n', sup_blocks)\n",
    "            print()\n",
    "            print()\n",
    "            print(unsup_input_nodes,'\\n', unsup_seeds, '\\n', unsup_blocks)\n",
    "            print()\n",
    "            print('___________________________________________________')\n",
    "            \n",
    "            # Load the input features as well as output labels\n",
    "            #batch_inputs, batch_labels = load_subtensor(train_g, seeds, input_nodes, device)\n",
    "            sup_blocks = [block.int() for block in sup_blocks]\n",
    "            sup_batch_inputs = sup_blocks[0].srcdata['features']\n",
    "            sup_batch_labels = sup_blocks[-1].dstdata['label']\n",
    "            \n",
    "            unsup_blocks = [block.int() for block in unsup_blocks]\n",
    "            unsup_batch_inputs = unsup_blocks[0].srcdata['features']\n",
    "            unsup_batch_labels = unsup_blocks[-1].dstdata['label']\n",
    "            \n",
    "            # Compute loss for each graph and add in additional term\n",
    "            sup_batch_pred = model(sup_blocks, sup_batch_inputs) #pass in our graph and features\n",
    "            sup_loss = loss_fcn(sup_batch_pred, sup_batch_labels) #compute loss from here\n",
    "            \n",
    "            unsup_batch_pred = model(unsup_blocks, unsup_batch_inputs)\n",
    "            unsup_loss = loss_fcn(unsup_batch_pred, unsup_batch_labels)\n",
    "            \n",
    "            additional_term = th.dist(sup_batch_pred, unsup_batch_pred,2)/args.batch_size\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65326, 512])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_batch_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Block(num_src_nodes=65326, num_dst_nodes=8620, num_edges=92996),\n",
       " Block(num_src_nodes=8620, num_dst_nodes=1024, num_edges=8319)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([282331, 164362, 664126,  ..., 784736, 284825, 279173])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(num_src_nodes=65326, num_dst_nodes=8620, num_edges=92996)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_blocks[0].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(num_src_nodes=65326, num_dst_nodes=8620, num_edges=92996)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_blocks[0].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65326, 512])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_blocks[0].srcdata['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_blocks[-1].dstdata['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
