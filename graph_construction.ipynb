{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Construction\n",
    "In this notebook we will construct the graph a bit differently from how we were doing it before to be more condusive to doing evaluation.  The approach will be as follows.\n",
    " - remove nodes with label of -1 --> was making data highly-balanced, skewing results\n",
    " - select a subset of nodes to do evaluation --> do not include these in any graph\n",
    " - build 2 graphs --> one connected based on ground truth (supervised), other on Jaccard (unsupervised)\n",
    " - evaluate periodically during training process as follows:\n",
    "     - one node at a time\n",
    "     - from holdout nodes, connect node to its original neighbors in the unsupervised\n",
    "     - pass in this node's two-hop subgraph through our network\n",
    "     - evaluate\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import dgl\n",
    "import scipy\n",
    "import networkx as nx\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in our data - ~1mil rows per dataset\n",
    "sup_neighbors = np.load('../hotel50k_graph_data/neighbors_sup.npy')\n",
    "unsup_neighbors = np.load('../hotel50k_graph_data/neighbors_unsup.npy')\n",
    "node_names = np.load('../hotel50k_graph_data/train_images_name.npy')\n",
    "node_features = np.load('../hotel50k_graph_data/db_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which nodes have label -1 --> remove these\n",
    "temp_node_names = node_names.tolist()\n",
    "temp_node_names = [int(s.split('/')[0]) for s in temp_node_names]\n",
    "\n",
    "#find all indeces (nodes) where node label is -1\n",
    "minus_one_indeces = [i for i,x in enumerate(temp_node_names) if x == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in our neighbors, make sure there are no nodes that have label -1 --> idenitfy all of these nodes, we will remove them later\n",
    "remap_dict = {x: -1 for x in minus_one_indeces}\n",
    "def mp(entry):\n",
    "    #go through our matrices, return -1 if node needs to be replaced or else just return the node\n",
    "    return remap_dict[entry] if entry in remap_dict else entry\n",
    "mp = np.vectorize(mp)\n",
    "\n",
    "#update our neighbor matrices so now we know which neighbors to ignore (ie. label -1)\n",
    "unsup_neighbors = mp(unsup_neighbors)\n",
    "sup_neighbors = mp(sup_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the number of nodes and neighbors as two variables for ease of reference\n",
    "#these dimensions are the same for the sup and unsup matrices\n",
    "n = unsup_neighbors.shape[0]\n",
    "m = unsup_neighbors.shape[1]\n",
    "index = list(range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Graph Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pandas to create an adjacency list as a dict of lists --> {0:[76,23,90], ...}\n",
    "unsup_df = pd.DataFrame(data = unsup_neighbors, index=index)\n",
    "\n",
    "#create adjacency list\n",
    "unsup_adj_list = unsup_df.T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all nodes that have label -1\n",
    "for k in minus_one_indeces:\n",
    "    del unsup_adj_list[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all possible neighbors that have label -1\n",
    "for k,v in unsup_adj_list.items():\n",
    "    v = [x for x in v if x!=-1]\n",
    "    unsup_adj_list[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract validation nodes\n",
    "from random import sample\n",
    "\n",
    "possible_val_keys = list(unsup_adj_list.keys())\n",
    "val_nodes = sample(possible_val_keys, 40000) #sample 40000 nodes for validation set ~10%\n",
    "\n",
    "#make sure no training nodes have neighbors from validation set\n",
    "#also make sure no training nodes have neighbors from validation set (important when we put them back in for evaluation)\n",
    "temp_val_nodes = set(val_nodes)\n",
    "for k,v in unsup_adj_list.items():\n",
    "    v = [x for x in v if x not in temp_val_nodes] #make sure no node can have neighbors from validation set\n",
    "    v = v[0:25]\n",
    "    unsup_adj_list[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract our validation nodes and their neighbors to which we will reconnect\n",
    "validation_nodes_and_neighbors = {k: unsup_adj_list[k] for k in val_nodes}\n",
    "\n",
    "#now remove the validation nodes from the unsupervised adjacency list\n",
    "for k in val_nodes:\n",
    "    del unsup_adj_list[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Graph Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_df = pd.DataFrame(data = sup_neighbors, index=index)\n",
    "\n",
    "#create adjacency list\n",
    "sup_adj_list = sup_df.T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all nodes that have label -1\n",
    "for k in minus_one_indeces:\n",
    "    del sup_adj_list[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all possible neighbors that have label -1\n",
    "for k,v in sup_adj_list.items():\n",
    "    v = [x for x in v if x!=-1]\n",
    "    sup_adj_list[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure no training nodes have neighbors from validation set\n",
    "#also make sure no training nodes have neighbors from validation set (important when we put them back in for evaluation)\n",
    "temp_val_nodes = set(val_nodes) #easier to search through --> O(1) time\n",
    "for k,v in sup_adj_list.items():\n",
    "    v = [x for x in v if x not in temp_val_nodes] #make sure no node can have neighbors from validation set\n",
    "    v = v[0:25]\n",
    "    sup_adj_list[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now remove the validation nodes from the supervised adjacency list\n",
    "for k in val_nodes:\n",
    "    del sup_adj_list[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Labels and Features based on Graphs\n",
    "Now that we've dropped all nodes with label -1 and have extracted nodes (and stored their neighbors) that will be used for validation, we need to properly alter our label vector and features matrix to reflect which nodes we've selected for training.  We also need to make sure we store the validation nodes, neighbors, and features because we'll need that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix weird case where there is no label 8 --> map label 92 to 8\n",
    "node_labels = np.array(temp_node_names)\n",
    "node_labels[node_labels==92] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new nested dict with following structure --> {node1: {neighbors:[], features:[], label:_},  node2:...}\n",
    "validation_dict = {}\n",
    "for k,v in validation_nodes_and_neighbors.items():\n",
    "    neighbors = v #store the neighbors of our node, k\n",
    "    label = node_labels[k] #get label\n",
    "    features = node_features[k] #get feature vector\n",
    "    \n",
    "    assert label != -1 #make sure we didn't mess up and have any label of -1\n",
    "    \n",
    "    #we will use/populate this dict as the value for each node in our validation set\n",
    "    temp_dict = {'neighbors':neighbors, 'features':features, 'label':label}\n",
    "    \n",
    "    #create entry in our validation dictionary\n",
    "    validation_dict[k] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our validation info as pickle file\n",
    "import pickle\n",
    "with open('validation_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(validation_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all rows (corresponds to nodes) of features and labels that correspond to labels of -1 or validation nodes\n",
    "#this way we'll be left with our train features and train labels\n",
    "indeces_to_drop = [*list(validation_dict.keys()), *list(minus_one_indeces)]\n",
    "train_features = np.delete(node_features,indeces_to_drop, axis = 0)\n",
    "train_labels = np.delete(node_labels,indeces_to_drop, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks for Correctness\n",
    "Now that we've created the necessary data structures to create our training graphs and validation data, let's heck to make sure everything is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we have the right number of nodes for each structure\n",
    "assert train_labels.shape[0] == train_features.shape[0] == len(sup_adj_list) == len(unsup_adj_list)\n",
    "\n",
    "#do we have the same nodes captured in the supervised and unsupervised graphs\n",
    "assert set(sup_adj_list.keys()) == set(set(unsup_adj_list.keys()))\n",
    "\n",
    "#make sure that there are no validation nodes in the training data\n",
    "assert len((set(sup_adj_list.keys()) | set(unsup_adj_list.keys())) & set(validation_dict.keys())) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dgl Remapping Problem\n",
    "DGL will atuomatically remap nodes to consecutive integers.  This is a problem because in our validation set, the nodes and their neighbors are mapped according to the original order.\n",
    "Solution\n",
    "- create networkx graph\n",
    "- create mapping dictionary of our current nodes to consecutive integers\n",
    "- save this mapping\n",
    "- apply this mapping to our validation nodes and their neighbors when we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remap unsupervised graph --> same should work for supervised graph\n",
    "unsup_g = nx.from_dict_of_lists(unsup_adj_list, nx.DiGraph)\n",
    "sup_g = nx.from_dict_of_lists(sup_adj_list, nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an ordered list of all of our nodes\n",
    "ordered_nodes = list(sorted(unsup_g.nodes))\n",
    "\n",
    "assert ordered_nodes == list(sorted(sup_g.nodes)) #quick check sup/unsup have same nodes\n",
    "assert list(unsup_g.nodes) == list(sorted(unsup_g.nodes)) #quick check that nodes were already in increasing order\n",
    "\n",
    "#create dict to map all nodes in our graphs to consecutive integers\n",
    "#we will need this same mapping dict to do the same for the \n",
    "remapping_dict = dict(zip(ordered_nodes, range(len(ordered_nodes))))\n",
    "\n",
    "#relabel graphs\n",
    "relabeled_unsup_g = nx.relabel_nodes(unsup_g, mapping = remapping_dict)\n",
    "relabeled_sup_g = nx.relabel_nodes(sup_g, mapping = remapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Validation Remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_data.pickle', 'rb') as f:\n",
    "    validation_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that the nodes we just remapped to matches what we do for validation\n",
    "remapped_validation_dict = {}\n",
    "i = max(relabeled_unsup_g) + 1 #counter for nodes\n",
    "\n",
    "for k,v in validation_dict.items():\n",
    "    k_new = i #indicate validation node\n",
    "    \n",
    "    v_new = v.copy()\n",
    "    \n",
    "    #remap neighbors\n",
    "    v_new['neighbors'] = [remapping_dict.get(item,item) for item in v_new['neighbors']]\n",
    "    \n",
    "    #populate our new validation dict\n",
    "    remapped_validation_dict[k_new] = v_new\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save validation info\n",
    "with open('final_validation_data.pickle', 'wb') as f:\n",
    "    pickle.dump(remapped_validation_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Graphs\n",
    "We have done our checks and have our data.  Now let's make our two graphs --> supervised and unsupervised.  We will also include features and labels for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dgl graph objects from networkx graphs\n",
    "unsup_dgl_graph = dgl.from_networkx(relabeled_unsup_g)\n",
    "sup_dgl_graph = dgl.from_networkx(relabeled_sup_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass in Features, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass in features to graph as a tensor\n",
    "#features extracted via MobileNet\n",
    "unsup_dgl_graph.ndata['features'] = th.tensor(train_features)\n",
    "sup_dgl_graph.ndata['features'] = th.tensor(train_features)\n",
    "\n",
    "#pass in labels\n",
    "#pass in labels to our graphs\n",
    "unsup_dgl_graph.ndata['labels'] = th.tensor(train_labels)\n",
    "sup_dgl_graph.ndata['labels'] = th.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save each graph object\n",
    "from dgl.data.utils import save_graphs\n",
    "graph_labels = {\"glabel\": th.tensor([0,1])}\n",
    "save_graphs(\"new_train_graphs.bin\", [unsup_dgl_graph, sup_dgl_graph], graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
