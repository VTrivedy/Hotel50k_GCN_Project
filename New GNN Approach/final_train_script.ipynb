{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSage Model Training\n",
    "Now that we've constructed the graphs, we can train on a simple 2-layer GraphSage network.  The procedure will be as follows:\n",
    "- Initialize our network\n",
    "- Send in mini-batches from both training graphs and save both of their outputs\n",
    "- Compute loss as follows: supervised_cross_entropy + unsupervised_cross_entropy + L2_distance(supervised_output, unsupervised_output) --> this is post softmax\n",
    "Evaluation will be done by adding in one node at a time from our validation nodes to our unsupervised graph.  From there, we will extract it's two-hop subgraph and pass this through our network to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import dgl\n",
    "import scipy\n",
    "import networkx as nx\n",
    "from progressbar import progressbar\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from dgl.data.utils import save_graphs, load_graphs, split_dataset\n",
    "\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.utils import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#load in previously computed graphs\n",
    "glist, label_dict = load_graphs(\"new_train_graphs.bin\")\n",
    "unsup_graph, sup_graph = glist[0], glist[1]\n",
    "\n",
    "#load in validation dictionary\n",
    "with open('final_validation_data.pickle', 'rb') as f:\n",
    "    validation_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 4823),\n",
       " (91, 2393),\n",
       " (0, 1917),\n",
       " (88, 1688),\n",
       " (84, 1371),\n",
       " (2, 1307),\n",
       " (3, 1290),\n",
       " (82, 1129),\n",
       " (83, 1037),\n",
       " (85, 951),\n",
       " (76, 853),\n",
       " (90, 791),\n",
       " (79, 761),\n",
       " (80, 747),\n",
       " (87, 691),\n",
       " (71, 676),\n",
       " (86, 655),\n",
       " (89, 650),\n",
       " (70, 650),\n",
       " (73, 641),\n",
       " (81, 640),\n",
       " (60, 631),\n",
       " (1, 549),\n",
       " (45, 548),\n",
       " (65, 546),\n",
       " (72, 543),\n",
       " (46, 494),\n",
       " (75, 449),\n",
       " (67, 435),\n",
       " (51, 424),\n",
       " (66, 413),\n",
       " (50, 391),\n",
       " (69, 390),\n",
       " (77, 386),\n",
       " (74, 382),\n",
       " (64, 356),\n",
       " (62, 338),\n",
       " (56, 337),\n",
       " (59, 332),\n",
       " (78, 324),\n",
       " (58, 321),\n",
       " (61, 298),\n",
       " (57, 280),\n",
       " (23, 270),\n",
       " (54, 267),\n",
       " (63, 251),\n",
       " (39, 242),\n",
       " (47, 208),\n",
       " (36, 195),\n",
       " (38, 194),\n",
       " (41, 194),\n",
       " (52, 184),\n",
       " (68, 183),\n",
       " (22, 177),\n",
       " (44, 172),\n",
       " (25, 150),\n",
       " (26, 149),\n",
       " (35, 141),\n",
       " (27, 134),\n",
       " (29, 133),\n",
       " (14, 130),\n",
       " (33, 117),\n",
       " (37, 112),\n",
       " (24, 107),\n",
       " (49, 98),\n",
       " (28, 92),\n",
       " (43, 92),\n",
       " (13, 85),\n",
       " (15, 79),\n",
       " (16, 75),\n",
       " (30, 74),\n",
       " (18, 74),\n",
       " (17, 73),\n",
       " (9, 73),\n",
       " (48, 67),\n",
       " (20, 62),\n",
       " (42, 60),\n",
       " (31, 57),\n",
       " (4, 51),\n",
       " (6, 49),\n",
       " (34, 48),\n",
       " (21, 45),\n",
       " (32, 45),\n",
       " (7, 42),\n",
       " (12, 35),\n",
       " (19, 27),\n",
       " (10, 23),\n",
       " (11, 15),\n",
       " (5, 15),\n",
       " (53, 4),\n",
       " (55, 1),\n",
       " (40, 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "lab_holder = []\n",
    "for k,v in validation_dict.items():\n",
    "    lab_holder.append(v['label'])\n",
    "counts = Counter(lab_holder)\n",
    "sorted(counts.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#add self loops to both graphs\n",
    "sup_graph = sup_graph.add_self_loop()\n",
    "unsup_graph = unsup_graph.add_self_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class StochasticTwoLayerGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = dgl.nn.SAGEConv(in_features, hidden_features, aggregator_type='pool')\n",
    "        self.conv2 = dgl.nn.SAGEConv(hidden_features, out_features, aggregator_type='pool')\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = F.relu(self.conv1(blocks[0], x))\n",
    "        x = F.relu(self.conv2(blocks[1], x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats1, hid_feats2, out_feats, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(\n",
    "            in_feats=in_feats, out_feats=hid_feats1, aggregator_type='pool')\n",
    "        self.conv2 = dglnn.SAGEConv(\n",
    "            in_feats=hid_feats1, out_feats=hid_feats2, aggregator_type='pool')\n",
    "        self.dense = nn.Linear(in_features=hid_feats2, out_features = out_feats)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, blocks, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = inputs\n",
    "        \n",
    "        h = self.conv1(blocks[0], h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        h = self.conv2(blocks[1], h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        h = self.dense(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        #note don't add a softmax layer to logits before cross_entropy --> this is done for us by F.crossentropy and causes overflow if we do\n",
    "        #cross_entropy needs to take in raw logits\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Setup\n",
    "- Load in precomputed subgraphs necessary to do evaluation --> will greatly speed up evaluation\n",
    "- Write code to evaluate validation nodes one subgraph at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def validation_evaluation(subgraph_holder, model):\n",
    "    val_1_correct = [] #store whether or not we correctly predicted the class for the 1-validation method\n",
    "    val_5_correct = [] #store whether our \n",
    "    \n",
    "    for subgraph in progressbar(subgraph_holder):\n",
    "        temp_input_nodes, temp_output_nodes, temp_blocks = subgraph[0], subgraph[1], subgraph[2] #unpack our values that we precomputed for each validation node\n",
    "        \n",
    "        #extract our features and labels to pass through the model and evaluate\n",
    "        temp_input_features = temp_blocks[0].srcdata['features']\n",
    "        temp_output_labels = temp_blocks[-1].dstdata['labels']\n",
    "\n",
    "        with th.no_grad():\n",
    "            model.eval()\n",
    "            temp_outputs = model(temp_blocks, temp_input_features)\n",
    "            _, temp_indices = th.max(temp_outputs, dim=1)\n",
    "            _, top_k_indices = th.topk(temp_outputs, 5, dim=1)\n",
    "            \n",
    "            correct_1 = temp_indices.item() == temp_output_labels.item() #did we get exact classification\n",
    "            correct_5 = temp_output_labels.item() in top_k_indices[0] #were our top 5 guesses one of the right classes\n",
    "            val_1_correct.append(correct_1)\n",
    "            val_5_correct.append(correct_5)\n",
    "\n",
    "    \n",
    "    #store and return tuple of total val_1/val_5 accuracies\n",
    "    val_1_accuracy = sum(val_1_correct)/len(val_1_correct)\n",
    "    val_5_accuracy = sum(val_5_correct)/len(val_5_correct)\n",
    "    return (val_1_accuracy, val_5_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Training Setup\n",
    "- set up our dataloaders to properly sample the same nodes from both graphs\n",
    "- if i only pass in 1024 node ids to both dataloaders and set the batch size to 1024 --> guarantee same nodes sampled\n",
    "    - need to split input nodes into equal batches of 1024 so I can feed in\n",
    "- pass through our model in minibatches and backprop loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#shuffle our node IDs\n",
    "train_nids = unsup_graph.nodes().tolist()\n",
    "random.shuffle(train_nids)\n",
    "\n",
    "#break up our node IDs into batches of 1024 --> we will pass each batch of IDs into our dataloader\n",
    "#this guarantees that we sample the same nodes from each graph\n",
    "BATCH_SIZE = 1024\n",
    "node_batches = [train_nids[i:i + BATCH_SIZE] for i in range(0, len(train_nids), BATCH_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (391 of 391) |######################| Elapsed Time: 0:30:16 Time:  0:30:16\n",
      "100% (40000 of 40000) |##################| Elapsed Time: 0:02:13 Time:  0:02:13\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:56 Time:  0:29:56\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:35 Time:  0:29:35\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:31 Time:  0:29:31\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:33 Time:  0:29:33\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:13 Time:  0:29:13\n",
      "100% (40000 of 40000) |##################| Elapsed Time: 0:02:13 Time:  0:02:13\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:10 Time:  0:29:10\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:07 Time:  0:29:07\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:02 Time:  0:29:02\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:53 Time:  0:28:53\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:54 Time:  0:28:54\n",
      "100% (40000 of 40000) |##################| Elapsed Time: 0:02:11 Time:  0:02:11\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:29:07 Time:  0:29:07\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:51 Time:  0:28:51\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:51 Time:  0:28:51\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:45 Time:  0:28:45\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:59 Time:  0:28:59\n",
      "100% (40000 of 40000) |##################| Elapsed Time: 0:02:13 Time:  0:02:13\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:44 Time:  0:28:44\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:43 Time:  0:28:43\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:39 Time:  0:28:39\n",
      "100% (391 of 391) |######################| Elapsed Time: 0:28:48 Time:  0:28:48\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "EVAL_EVERY = 1 #evaluate every n epochs\n",
    "RECORD_EVERY = 30 #record every n minibatch results \n",
    "\n",
    "#set up model parameters\n",
    "in_features = unsup_graph.ndata['features'].shape[1]\n",
    "out_features = len(set(unsup_graph.ndata['labels'].tolist())) #how many unique classes = number of outputs\n",
    "hidden_features_1 = 256\n",
    "hidden_features_2 = 128\n",
    "\n",
    "#load in all the subgraphs we need for evaluation\n",
    "subgraph_holder = th.load('val_subgraphs/full_subgraph_list')\n",
    "\n",
    "model = SAGE(in_features, hidden_features_1, hidden_features_2, out_features, 0.3)\n",
    "opt = th.optim.Adam(model.parameters())\n",
    "\n",
    "#sample 15 1-hop neighbors and 10 2-hop neighbors for each node\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    for step, train_nids in enumerate(progressbar(node_batches)):\n",
    "        model = model.train() #set train context\n",
    "        \n",
    "        #set up our sampler and dataloaders --> one dataloader per graph\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler([15,10])\n",
    "        sup_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "            sup_graph, train_nids, sampler,\n",
    "            batch_size=1024,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=0)\n",
    "        unsup_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "            unsup_graph, train_nids, sampler,\n",
    "            batch_size=1024,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=0)\n",
    "\n",
    "        #generate graph dependency for each batch of nodes for both graphs\n",
    "        sup_input_nodes, sup_output_nodes, sup_blocks = next(iter(sup_dataloader))\n",
    "        unsup_input_nodes, unsup_output_nodes, unsup_blocks = next(iter(unsup_dataloader))\n",
    "        \n",
    "        \n",
    "        #make sure the same nodes were sampled\n",
    "        assert list(sorted(sup_output_nodes.tolist())) == list(sorted(unsup_output_nodes.tolist()))\n",
    "\n",
    "        \n",
    "        #extract our features and labels for supervised/unsupervised graphs\n",
    "        sup_input_features = sup_blocks[0].srcdata['features']\n",
    "        sup_output_labels = sup_blocks[-1].dstdata['labels']\n",
    "        unsup_input_features = unsup_blocks[0].srcdata['features']\n",
    "        unsup_output_labels = unsup_blocks[-1].dstdata['labels']\n",
    "\n",
    "        \n",
    "        \n",
    "        #pass each subgraph through our model to get our predictions\n",
    "        sup_output_predictions = model(sup_blocks, sup_input_features)\n",
    "        unsup_output_predictions = model(unsup_blocks, unsup_input_features)\n",
    "\n",
    "        \n",
    "        \n",
    "        #get the training accuracy for both graphs\n",
    "        _, sup_indices = th.max(sup_output_predictions, dim=1)\n",
    "        sup_correct = th.sum(sup_indices == sup_output_labels)\n",
    "        sup_train_accuracy = sup_correct.item() * 1.0 / len(sup_output_labels)\n",
    "\n",
    "        _, unsup_indices = th.max(unsup_output_predictions, dim=1)\n",
    "        unsup_correct = th.sum(unsup_indices == unsup_output_labels)\n",
    "        unsup_train_accuracy = unsup_correct.item() * 1.0 / len(unsup_output_labels)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #reorder results so vectors correspond to the same nodes in both graphs --> necessary to do the Euclidean distance later\n",
    "        sup_ids = sup_blocks[1].dstdata['_ID'] \n",
    "        unsup_ids = unsup_blocks[1].dstdata['_ID']\n",
    "        sorted_sup_vals, sorted_sup_indices = sup_ids.sort() #how to reorder the supervised predictions/labels\n",
    "        sorted_unsup_vals, sorted_unsup_indices = unsup_ids.sort() #how to reorder the unsupervised predictions/labels\n",
    "        \n",
    "        #reorder our labels and output predictions according to their corresponding node IDs\n",
    "        sup_output_predictions = sup_output_predictions[sorted_sup_indices]\n",
    "        unsup_output_predictions = unsup_output_predictions[sorted_unsup_indices]\n",
    "        sup_output_labels = sup_output_labels[sorted_sup_indices]\n",
    "        unsup_output_labels = unsup_output_labels[sorted_unsup_indices]       \n",
    "        \n",
    "        #check to make sure the labels match up --> if the nodes are in the same order then the labels should be too\n",
    "        assert th.equal(sup_output_labels,unsup_output_labels)\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        #compute the loss for both results, calculate the distance between their softmax outputs, then combine these values\n",
    "        sup_loss = F.cross_entropy(sup_output_predictions, sup_output_labels)\n",
    "        unsup_loss = F.cross_entropy(unsup_output_predictions, unsup_output_labels)\n",
    "        distance_term = th.dist(F.softmax(sup_output_predictions,dim=1), F.softmax(unsup_output_predictions,dim=1))/BATCH_SIZE\n",
    "        loss = (sup_loss + unsup_loss)/2 + distance_term\n",
    "\n",
    "        #backprop through network to update weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        with io.capture_output() as captured: #suppress weird printing from files\n",
    "            #record statistics for mini batches\n",
    "            if step%RECORD_EVERY==0:\n",
    "                with open('logfiles/minibatch_log_file.txt', 'a') as f:\n",
    "                    f.write('Epoch {:04d} | Step {:05d} | Total Loss {:.4f} | Sup Loss {:.4f} | Unsup Loss {:.4f} | Distance Term {:.4f} | Sup Train Acc {:.4f} | Unsup Train Acc {:.4f}\\n'.format(\n",
    "                    epoch, step, loss, sup_loss, unsup_loss, distance_term, sup_train_accuracy, unsup_train_accuracy));\n",
    "\n",
    "    \n",
    "    with io.capture_output() as captured: #suppress weird printing from files\n",
    "        #record epoch time\n",
    "        end_time = time.time()\n",
    "        with open('logfiles/minibatch_log_file.txt', 'a') as f:\n",
    "            f.write(f'\\nEpoch Time: {end_time-start_time} seconds\\n\\n');\n",
    "\n",
    "        #evaluate on validation set and record results     \n",
    "        if epoch%EVAL_EVERY==0:\n",
    "            #extract our evaluation metrics\n",
    "            results = validation_evaluation(subgraph_holder, model)\n",
    "            val_1_accuracy, val_5_accuracy = results[0], results[1]\n",
    "\n",
    "            with open('logfiles/minibatch_log_file.txt', 'a') as f:\n",
    "                f.write('\\n\\n\\n________________________________________________\\n');\n",
    "                f.write(f'VALIDATION SET EVALUATION\\n');\n",
    "                f.write(f'Validation 1 Accuracy: {val_1_accuracy}\\nValidation 5 Accuracy: {val_5_accuracy}\\n');\n",
    "                f.write('\\n________________________________________________\\n\\n\\n');\n",
    "        \n",
    "\n",
    "        #Save model\n",
    "        th.save(model.state_dict(), f'minibatch_models/model{epoch}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
